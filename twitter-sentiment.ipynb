{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mishj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')) #words that dont have much importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "column_names=['target','id','date','flag','user','text']\n",
    "twitter_data=pd.read_csv('twitter.csv',encoding='ISO-8859-1',header=None,names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the distribution of traget values\n",
    "twitter_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "twitter_data.replace({'target':{4:1}},inplace=True)\n",
    "print(twitter_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0=-ve\n",
    "#1=+ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming->to reduce to the root word\n",
    "port_stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content=re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content=stemmed_content.lower()\n",
    "    stemmed_content=stemmed_content.split()\n",
    "    stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content=' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_data['stemmed_content']=twitter_data['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_data.to_pickle(\"stemmed_content.pkl\")\n",
    "\n",
    "# # Load\n",
    "twitter_data_stemmed = pd.read_pickle(\"stemmed_content.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          switchfoot http twitpic com zl awww bummer sho...\n",
       "1          upset updat facebook text might cri result sch...\n",
       "2          kenichan dive mani time ball manag save rest g...\n",
       "3                            whole bodi feel itchi like fire\n",
       "4                              nationwideclass behav mad see\n",
       "                                 ...                        \n",
       "1599995                           woke school best feel ever\n",
       "1599996    thewdb com cool hear old walt interview http b...\n",
       "1599997                         readi mojo makeov ask detail\n",
       "1599998    happi th birthday boo alll time tupac amaru sh...\n",
       "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
       "Name: stemmed_content, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "1599995    1\n",
      "1599996    1\n",
      "1599997    1\n",
      "1599998    1\n",
      "1599999    1\n",
      "Name: target, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=twitter_data_stemmed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=twitter_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting textudal data to numerical data\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100000, \n",
    "    ngram_range=(1,3),  # Include unigrams, bigrams, and trigrams\n",
    "    sublinear_tf=True  # Helps large datasets\n",
    ")\n",
    "\n",
    "X_train=vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 12249876 stored elements and shape (1280000, 100000)>\n",
      "  Coords\tValues\n",
      "  (0, 92776)\t0.226730546053314\n",
      "  (0, 71690)\t0.29843494609144333\n",
      "  (0, 42020)\t0.438964228477445\n",
      "  (0, 19967)\t0.31220992695572786\n",
      "  (0, 47581)\t0.349303261118467\n",
      "  (0, 95621)\t0.37301380683658636\n",
      "  (0, 93234)\t0.5551694711946487\n",
      "  (2, 19967)\t0.28408039996898515\n",
      "  (2, 21980)\t0.13829596346277317\n",
      "  (2, 84157)\t0.13674485997939545\n",
      "  (2, 23899)\t0.21250063834654384\n",
      "  (2, 83524)\t0.19865348118804999\n",
      "  (2, 91274)\t0.24094520499895222\n",
      "  (2, 13246)\t0.22865250952061522\n",
      "  (2, 95653)\t0.2447461135564103\n",
      "  (2, 53836)\t0.17631450558588618\n",
      "  (2, 84843)\t0.1108709091128146\n",
      "  (2, 40668)\t0.11833198086211333\n",
      "  (2, 31594)\t0.13743563887372914\n",
      "  (2, 24942)\t0.14829569231078027\n",
      "  (2, 58041)\t0.12269429245431486\n",
      "  (2, 22169)\t0.20408875639069057\n",
      "  (2, 20032)\t0.3185693847047648\n",
      "  (2, 85308)\t0.25762467608740774\n",
      "  (2, 83760)\t0.21758909179592545\n",
      "  :\t:\n",
      "  (1279997, 55004)\t0.3375482644123121\n",
      "  (1279997, 20430)\t0.5873071469288784\n",
      "  (1279997, 55005)\t0.631543495112163\n",
      "  (1279998, 86563)\t0.16474965307949144\n",
      "  (1279998, 16891)\t0.12703320626342485\n",
      "  (1279998, 38599)\t0.15558960719641615\n",
      "  (1279998, 22450)\t0.1885908846910817\n",
      "  (1279998, 91478)\t0.16681008945991954\n",
      "  (1279998, 33770)\t0.16599639683713013\n",
      "  (1279998, 33815)\t0.24225241362861769\n",
      "  (1279998, 34546)\t0.18349738636272628\n",
      "  (1279998, 36701)\t0.20292145657700986\n",
      "  (1279998, 55680)\t0.22638652369491363\n",
      "  (1279998, 79440)\t0.24001248067046277\n",
      "  (1279998, 38685)\t0.29144979395696935\n",
      "  (1279998, 91583)\t0.3042018584804513\n",
      "  (1279998, 17651)\t0.33393501426567357\n",
      "  (1279998, 36739)\t0.3846346903802621\n",
      "  (1279998, 79461)\t0.40278453025965916\n",
      "  (1279999, 49823)\t0.20694379918403\n",
      "  (1279999, 5771)\t0.2630512626674065\n",
      "  (1279999, 91816)\t0.33507348784315355\n",
      "  (1279999, 91277)\t0.3814081740878371\n",
      "  (1279999, 50512)\t0.5498144863192803\n",
      "  (1279999, 18010)\t0.5726568696046832\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 0.7889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.77      0.78    160000\n",
      "    positive       0.78      0.81      0.79    160000\n",
      "\n",
      "    accuracy                           0.79    320000\n",
      "   macro avg       0.79      0.79      0.79    320000\n",
      "weighted avg       0.79      0.79      0.79    320000\n",
      "\n",
      "\n",
      "Naïve Bayes (MultinomialNB) Accuracy: 0.7696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77    160000\n",
      "    positive       0.77      0.77      0.77    160000\n",
      "\n",
      "    accuracy                           0.77    320000\n",
      "   macro avg       0.77      0.77      0.77    320000\n",
      "weighted avg       0.77      0.77      0.77    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000,C=1),\n",
    "    \"Naïve Bayes (MultinomialNB)\": MultinomialNB(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Params: {'C': 1, 'max_iter': 100, 'solver': 'liblinear'}\n",
      "0.7764789060391557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "log_reg_params = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"max_iter\": [100, 300, 500]\n",
    "}\n",
    "log_reg = GridSearchCV(LogisticRegression(), log_reg_params, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(f\"Best Logistic Regression Params: {log_reg.best_params_}\")\n",
    "print(log_reg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(C=50, kernel='linear', class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
